{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35c55ce",
   "metadata": {},
   "source": [
    "# Image Classifier - <i><font color=orange>01 Feature extraction</font></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58150b32",
   "metadata": {},
   "source": [
    "To start with, we need to extract high level features for our training, validation and test images. We will use the MobileNet V2 convolutional neural network to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c22836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# disable recognition of CUDA capable graphics card to avoid library updates. We will stick to CPU processing for this exercise.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a07cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create image generator\n",
    "# We will use rescale on all the generators to rescale the image data between 0 and 1. TensorFlow Hub image modules require images normalized between zero and one.\n",
    "# horizontal_flip and rotation_range will provide slightly altered images which helps in avoiding overfitting the training data.\n",
    "# we will not use validation_split as we are already provided with a separate validation set for this exercise.\n",
    "train_generator = ImageDataGenerator(rescale=1/255, horizontal_flip=True, rotation_range=5)\n",
    "valid_generator = ImageDataGenerator(rescale=1/255)\n",
    "test_generator = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448d4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n",
      "Found 139 images belonging to 6 classes.\n",
      "Found 50 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the Train, validation and test sets using the ImageDataGenerator objects.\n",
    "# We resize the images to 224x224 as required by the mobilenet v2 convnet for feature extraction.\n",
    "# NOTE: We set shuffle=False to get consistent outcomes for this exercise over multiple runs.\n",
    "trainset = train_generator.flow_from_directory(\n",
    "    'train', batch_size=32, target_size=(224, 224),\n",
    "    shuffle=False)\n",
    "validset = valid_generator.flow_from_directory(\n",
    "    'valid', batch_size=32, target_size=(224, 224),\n",
    "    shuffle=False)\n",
    "testset = test_generator.flow_from_directory(\n",
    "    'test', batch_size=32, target_size=(224, 224),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b9846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Create and prepare the graph for feature extraction.  \n",
    "# ref: tensorflow-ecosystem-v1/units/transfer-learning-v1\n",
    "\n",
    "# Create graph\n",
    "img_graph = tf.Graph()\n",
    "\n",
    "with img_graph.as_default():\n",
    "    # Download mobilenet v2 module\n",
    "    module_url = 'https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/2'\n",
    "    feature_extractor = hub.Module(module_url)\n",
    "    \n",
    "    # Create input placeholder\n",
    "    input_imgs = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3])\n",
    "    \n",
    "    # A node with the features\n",
    "    imgs_features = feature_extractor(input_imgs)\n",
    "    \n",
    "    # Collect initializers\n",
    "    init_op = tf.group([\n",
    "        tf.global_variables_initializer(), tf.tables_initializer()\n",
    "    ])\n",
    "\n",
    "# Finalize the graph to make it read-only and prevent new operations.\n",
    "img_graph.finalize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8681e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing:  training\n",
      "processed batch  0 features shape:  (32, 1280)\n",
      "processed batch  1 features shape:  (32, 1280)\n",
      "processed batch  2 features shape:  (32, 1280)\n",
      "processed batch  3 features shape:  (32, 1280)\n",
      "processed batch  4 features shape:  (32, 1280)\n",
      "processed batch  5 features shape:  (32, 1280)\n",
      "processed batch  6 features shape:  (32, 1280)\n",
      "processed batch  7 features shape:  (32, 1280)\n",
      "processed batch  8 features shape:  (24, 1280)\n",
      "\n",
      "processing:  validation\n",
      "processed batch  0 features shape:  (32, 1280)\n",
      "processed batch  1 features shape:  (32, 1280)\n",
      "processed batch  2 features shape:  (32, 1280)\n",
      "processed batch  3 features shape:  (32, 1280)\n",
      "processed batch  4 features shape:  (11, 1280)\n",
      "\n",
      "processing:  testing\n",
      "processed batch  0 features shape:  (32, 1280)\n",
      "processed batch  1 features shape:  (18, 1280)\n"
     ]
    }
   ],
   "source": [
    "# We will now loop through the images in batches and extract features from the downloaded module. \n",
    "# This will provide us with 1280 features for every image. We will use these features as our X (input) values for all our models except the ConvNet.\n",
    "\n",
    "# Create a session\n",
    "sess = tf.Session(graph=img_graph)\n",
    "\n",
    "# Initialize it\n",
    "sess.run(init_op)\n",
    "\n",
    "# Extract features in batches\n",
    "# lets create lists to store the raw images, extracted features and hot-encoded labels during the extraction process.\n",
    "\n",
    "# this will hold the image data\n",
    "imgs_train = []\n",
    "imgs_valid = []\n",
    "imgs_test = []\n",
    "\n",
    "# this will hold the extracted features\n",
    "x_train = []\n",
    "x_valid = []\n",
    "x_test = []\n",
    "\n",
    "# this will hold the one hot encoded labels \n",
    "y_train = []\n",
    "y_valid = []\n",
    "y_test = []\n",
    "\n",
    "# run a loop over all the 3 types of dataset - and perform feature extraction\n",
    "# provide 3 lists for training, validation and testing sets. Each containing a label for printing, input set and 3 output lists\n",
    "for label, data, x_imgs, x, y,  in [['training', trainset, imgs_train, x_train, y_train], ['validation', validset, imgs_valid, x_valid, y_valid], ['testing', testset, imgs_test, x_test, y_test]]:\n",
    "    print(\"\\nprocessing: \", label) \n",
    "    for i in range(len(data)):\n",
    "        # note that data is processed in batches of 32 as configured during image loading using flow_from_directory.\n",
    "        batch_imgs, y_ohe_label = data.next()\n",
    "\n",
    "        # Extract batches of features\n",
    "        features = sess.run(imgs_features, feed_dict={input_imgs: batch_imgs})\n",
    "        \n",
    "        # append batch items to output lists (use extend instead of append to append list items instead of list objects)\n",
    "        x_imgs.extend(batch_imgs)\n",
    "        x.extend(features)\n",
    "        y.extend(y_ohe_label)\n",
    "\n",
    "        print(\"processed batch \", i, \"features shape: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd1ea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike': 0, 'car': 1, 'motorcycle': 2, 'other': 3, 'truck': 4, 'van': 5}\n",
      "{0: 'bike', 1: 'car', 2: 'motorcycle', 3: 'other', 4: 'truck', 5: 'van'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lets print the class indices\n",
    "print(trainset.class_indices)\n",
    "\n",
    "# create an inverse dictionary of class_indices for lookup. This will help us to retrieve a lable using the numeric identifier as the key.\n",
    "# ref: https://stackoverflow.com/questions/2568673/inverse-dictionary-lookup-in-python\n",
    "ivd = {v: k for k, v in trainset.class_indices.items()}\n",
    "print(ivd)\n",
    "\n",
    "# To have easy access to lables for every input item, lets create and fill a list of labels for training, validation and test sets. \n",
    "labels_train = []\n",
    "labels_valid = []\n",
    "labels_test = []\n",
    "\n",
    "# We will also store the numeric values as some of the models work better with numeric identifiers rather than one-hot-encoded ones.\n",
    "y_train_int = []\n",
    "y_valid_int = []\n",
    "y_test_int = []\n",
    "\n",
    "# iterate train, valid and test y arrays to fill text label arrays for all \n",
    "for y, labels_list, y_int_list in [[y_train, labels_train, y_train_int], [y_valid, labels_valid, y_valid_int], [y_test, labels_test, y_test_int]]:\n",
    "    # note that y contains a list of one hot encoded output values. we need to convert them to indices for our lookup with our 'ivd' dictionary. \n",
    "    for v in y:\n",
    "        # find the index where value in the one hot encoded list is 1. \n",
    "        # ref: https://stackoverflow.com/questions/42497340/how-to-convert-one-hot-encodings-into-integers\n",
    "        match = np.where(v == 1) # provides a list of matches, we know there is only one match for 1 in a one hot encoded list.\n",
    "        first = match[0]         # get the first item which is an array with the value and its dtype, viz. array([5], dtype=int64)\n",
    "        index = first[0]         # get the first item which is the numeric index.\n",
    "        \n",
    "        # fetch the label from the inverse dictionary\n",
    "        label = ivd[index]\n",
    "        \n",
    "        # append the label to our label list\n",
    "        labels_list.append(label)\n",
    "        \n",
    "        # append the index to our int list\n",
    "        y_int_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5badb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the processed lists and supporting data in npz format\n",
    "# ref: k-nearest-neighbors-v2/units/pickle-and-numpy-formats-v2\n",
    "\n",
    "# save the training, validation and testing data in separate npz files. \n",
    "np.savez(\"training.npz\", images=imgs_train, features=x_train, targets_ohe=y_train, targets_int=y_train_int, labels=labels_train)\n",
    "np.savez(\"validation.npz\", images=imgs_valid, features=x_valid, targets_ohe=y_valid, targets_int=y_valid_int, labels=labels_valid)\n",
    "np.savez(\"testing.npz\", images=imgs_test, features=x_test, targets_ohe=y_test, targets_int=y_test_int, labels=labels_test)\n",
    "\n",
    "# save the labels dictionary for inverse lookup \n",
    "with open('labels_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(ivd, f)\n",
    "    \n",
    "# also create and save a results dictionary for storing all the accuracy metrics that we will collect along the way.\n",
    "# we will display and analyze these metrics in our last segment.\n",
    "results = {}\n",
    "with open('results_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
